{\rtf1\ansi\ansicpg1252\cocoartf1347\cocoasubrtf570
{\fonttbl\f0\fnil\fcharset0 SegoeUI;\f1\froman\fcharset0 TimesNewRomanPSMT;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue233;}
\margl1440\margr1440\vieww15700\viewh18840\viewkind1\viewscale110
\deftab720
\pard\pardeftab720\ri0

\f0\fs22 \cf0 Exp, Review\
(3) 2\
(3) 2\
(2) 4\
(4) 1\
\
Dear Rowdy Howell -\
\
We are sorry to inform you that your submission\
\
\'a0575: "AirWare: Large Vocabulary, In-air Gesture Recognition with IR Proximity and Audio Doppler on Smartphones"\
\
has not been accepted for publication at UbiComp 2015. The final reviews for your submission are included below. We sincerely hope these will be of use in reformulating your work for future publication. \
\
The decisions as to which submissions to accept were made after careful reviews by a number of highly qualified experts from around the world. We had to be very selective. Of 394 submissions, only 23.6% were chosen for acceptance.\
\
We would very much like to encourage you to consider submission of your work to other tracks of UbiComp 2015. Demos and posters are a great opportunity for presenting your work in the conference and submissions to these tracks will be archived in the ACM Digital \'a0Library. The closing date for submission to these tracks is May 29. There are also workshops at UbiComp 2015, which offer a chance to contribute to and discuss a focused topic with others interested in that area. Please see the following links for full details.\
\
\pard\pardeftab720\ri0
{\field{\*\fldinst{HYPERLINK "https://webmail.smu.edu/owa/redir.aspx?SURL=-4Ygp"}}{\fldrslt \cf2 \ul \ulc2 http://www.ubicomp.org/ubicomp2015/calls/demos.php}}\
http://www.ubicomp.org/ubicomp2015/calls/posters.php\
http://www.ubicomp.org/ubicomp2015/attending/workshops.php\
\
\
Thanks you again for submitting your work to UbiComp 2015. Although we were not able to accept the submission, we hope you will be able to take advantage of the feedback in the review and consider revising and submitting the work to other tracks this year. And, we hope you will be able to join us in Osaka this September!\
\
Tanzeem Choudhury\
Hans Gellersen\
Koji Yatani\
\
UbiComp 2015 PC Chairs\
\
\
::::: Reviews for 575 - AirWare: Large Vocabulary, In-air Gesture Recognition with IR Proximity and Audio Doppler on Smartphones :::::\
\
------------------------ Submission 575, Review 1 ------------------------\
\
Title: AirWare: Large Vocabulary, In-air Gesture Recognition with IR Proximity and Audio Doppler on Smartphones\
\
\
Confidence\
\
\'a0\'a03 \'a0(Very confident - I am knowledgeable in the area)\
\
Overall Rating\
\
\'a0\'a02 \'a0(Probably reject: I would argue for rejecting this paper.)\
\
Contribution to UbiComp\
\
\'a0\'a0The paper presents a mid-air gesture input approach for mobile devices\
\'a0\'a0based on an IR proximity sensor and an audio doppler effect.\
\
The Review\
\
\'a0\'a0Although there is one relatively high rating, most of the reviewers think\
\'a0\'a0that the paper is not ready yet. Even the reviewer who gave a four (with\
\'a0\'a0expertise 2) has some concerns about this paper. \
\
\'a0\'a0Summarzing, I think that the paper provides an interesting topic for the\
\'a0\'a0Ubicomp community. On the other side, it is still unclear in which way\
\'a0\'a0this paper differs from related work. As mentioned by R4, a lot of\
\'a0\'a0similar research has been done over the last three years. The most\
\'a0\'a0critical thing of this paper, however, is the low accuracy with 35%\
\'a0\'a0recognition rate. This seems to be very, very low. The paper would also\
\'a0\'a0benefit by providing more technical details (e.g. R4: explain how the\
\'a0\'a0proximity sensor is used etc.). \
\
\'a0\'a0Saying that, I also agree with the reviewers comments that this paper is\
\'a0\'a0not ready yet. As a work-in-progress, it might be highly interesting -\
\'a0\'a0also to get feedback. \
\
\
------------------------ Submission 575, Review 2 ------------------------\
\
Title: AirWare: Large Vocabulary, In-air Gesture Recognition with IR Proximity and Audio Doppler on Smartphones\
\
\
Confidence\
\
\'a0\'a03 \'a0(Very confident - I am knowledgeable in the area)\
\
Overall Rating\
\
\'a0\'a02 \'a0(Probably reject: I would argue for rejecting this paper.)\
\
Contribution to UbiComp\
\
\'a0\'a0This paper proposes using the IR sensor along with Doppler shift\
\'a0\'a0information via audio pings from a microphone and speaker to identify in\
\'a0\'a0air gestures in above a commercial smart phone. The authors demonstrate 9\
\'a0\'a0gestures with 65% accuracy and 21 gestures with 35% accuracy.\
\
The Review\
\
\'a0\'a0The major draw back of this paper is that the accuracy of detecting an in\
\'a0\'a0air gesture is too poor for any traditional human computer interface\
\'a0\'a0usage scenario. It would greatly improve this work if the authors could\
\'a0\'a0A) improve the accuracy (possibly by using additional sensors) or B)\
\'a0\'a0demonstrate a convincing application that Airwave can enable with only a\
\'a0\'a065% or 35% chance of accurately detecting a gesture.\
\
\
------------------------ Submission 575, Review 3 ------------------------\
\
Title: AirWare: Large Vocabulary, In-air Gesture Recognition with IR Proximity and Audio Doppler on Smartphones\
\
\
Confidence\
\
\'a0\'a02 \'a0(Somewhat confident - I have passing knowledge)\
\
Overall Rating\
\
\'a0\'a04 \'a0(Maybe accept: I would agree with accepting this paper.)\
\
Contribution to UbiComp\
\
\'a0\'a0The main contribution that this paper makes is showing how Doppler\
\'a0\'a0gesture sensing techniques with the embedded IR proximity sensor input to\
\'a0\'a0achieve a large vocabulary of in-air gestures. This will be of interest\
\'a0\'a0to other researchers doing gesture based interaction.\
\
The Review\
\
\'a0\'a0In this paper the author show how Doppler gesture sensing techniques can\
\'a0\'a0be combined with the embedded IR proximity sensor on a mobile phone to\
\'a0\'a0enable in-air gesture input. The paper is interesting and seems to have a\
\'a0\'a0promising technique, but there are short comings in the user evaluation\
\'a0\'a0and other parts. So I think it should be maybe accepted.\
\
\'a0\'a0The author's being the paper with a good description of their motivation\
\'a0\'a0and other related work that has been done to support in-air gestures.\
\'a0\'a0They point out the limitations of these systems, particularly in\
\'a0\'a0supporting a rich gesture vocabulary for in-air gestures. They clearly\
\'a0\'a0explain what the novel contributions of this paper are relative to this\
\'a0\'a0earlier work.\
\
\'a0\'a0The author provides a good description of the technical approach for\
\'a0\'a0collecting gesture features from Doppler effects and using the phone's IR\
\'a0\'a0sensor. This seems like \'a0a very promising method. However it was unclear\
\'a0\'a0if these features could be used to track the hand position of the use or\
\'a0\'a0were abstract representations of the gesture motion needed by the machine\
\'a0\'a0learning algorithms. If the users hand motions can be tracked from the\
\'a0\'a0collected features then it would be good to mention how accurate this\
\'a0\'a0tracking is. \
\
\'a0\'a0In describing the user experiment it would have been good to provide more\
\'a0\'a0images of some of the more unusual gestures (such as wobble), so that the\
\'a0\'a0reader can understand the full set of gestures testing. \
\
\'a0\'a0The within subject performance for the gesture recognition was quite low\
\'a0\'a0at only 35% for the full gesture set. It would be good for the author to\
\'a0\'a0include more discussion about how this could be improved from the\
\'a0\'a0software side. In the discussion section they talk about how low gesture\
\'a0\'a0recognition might be due to limitations in the experimental methodology,\
\'a0\'a0but they don't talk at all about how the machine learning could be\
\'a0\'a0improved or other algorithmic improvement.\
\
\
------------------------ Submission 575, Review 4 ------------------------\
\
Title: AirWare: Large Vocabulary, In-air Gesture Recognition with IR Proximity and Audio Doppler on Smartphones\
\
\
Confidence\
\
\'a0\'a04 \'a0(Highly confident - I consider myself an expert in the area)\
\
Overall Rating\
\
\'a0\'a01 \'a0(Definite reject: I would argue strongly for rejecting this paper.)\
\
Contribution to UbiComp\
\
\'a0\'a0The paper presents an approach to use in-air gestures for interacting\
\'a0\'a0with mobile devices. It uses doppler shifts in the audio signal to detect\
\'a0\'a0the direction of the gestures. It also adds the information from the\
\'a0\'a0infrared proximity sensor on the device to improve the gesture\
\'a0\'a0dictionary. Overall I think this is area of interest for the community\
\'a0\'a0but right now the performance of the system is too low to be considered\
\'a0\'a0for publication.\
\
The Review\
\
\'a0\'a0The authors present an interesting way of leveraging the inaudible audio\
\'a0\'a0signals to detect in-air gestures. Similar works have been published in\
\'a0\'a0last 3 years at different venues and are definitely of interest to the\
\'a0\'a0community. The performance of the system is too low right now (35% and\
\'a0\'a065%) to be published. It might be a better fit for a work in progress at\
\'a0\'a0a conference.\
\
\'a0\'a0I do not recommend the paper to be accepted right now but I would like to\
\'a0\'a0use this as an opportunity to suggest some changes and directions that\
\'a0\'a0might improve this work in future. \
\
\'a0\'a01. The use of proximity sensor needs to be described more clearly. It is\
\'a0\'a0important to explain how the proximity sensor is used not just for the\
\'a0\'a0algorithm but also as part of the interaction. I think the use of this\
\'a0\'a0sensor can prove to be a differentiating factor for this paper, in\
\'a0\'a0comparison to previous explorations. \
\
\'a0\'a02. The paper lacks an illustration and/or video of the 21 gestures. \
\
\'a0\'a03. The authors need to explain why is the technology working and why is\
\'a0\'a0the signal different for different sensors and how are they contributing\
\'a0\'a0to the overall solution. Try not to simply treat it as a machine\
\'a0\'a0learning-based black box. A more informed approach and explanation might\
\'a0\'a0actually improve the performance of the system as well. \
\
\'a0\'a04. Try to remove the need to train a model as well. A quick look at the\
\'a0\'a0cited papers [1,4,8] shows that those works did not need any training. It\
\'a0\'a0is especially less desirable to train a per user model. That said, if the\
\'a0\'a0machine learning approach leads to significantly improved performance for\
\'a0\'a0a larger gesture-set then it might be a justifiable approach. \
\
\'a0\'a0Overall, unfortunately I cannot recommend to accept the paper in its\
\'a0\'a0current form but I hope that this and other reviews would help the\
\'a0\'a0authors in improving their work significantly.
\f1\fs12 \
}